{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidedomini/proximal-policy-optimization-implementation/blob/main/DL_Project_PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20kEZ_IiArFZ"
      },
      "source": [
        "# **Proximal Policy Optimization (PPO) implementation**\n",
        "\n",
        "University of Bologna - Department of Computer Science and Engineering\n",
        "\n",
        "Project for ***Deep Learning*** course\n",
        "\n",
        "Author: Davide Domini - davide.domini@studio.unibo.it - 0001025049\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50eCclm1BLBX"
      },
      "source": [
        "## **Important remark**\n",
        "\n",
        "Since there exist several variants of the PPO algorithm, in this project I chose to implement a Clip-based version with pseudocode found in [OpenAI Spinning Up documentation](https://spinningup.openai.com/en/latest/algorithms/ppo.html#id7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFBIj_3xFM-h"
      },
      "source": [
        "## **Tools**\n",
        "\n",
        "Several tools were used to implement this project, namely:\n",
        "- PyTorch\n",
        "- OpenAI Gym\n",
        "- OpenCV\n",
        "- Numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5-2YPRbKaA3"
      },
      "source": [
        "## **Preliminary operations**\n",
        "\n",
        "The following code downloads all the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGCsuVXgAmoD"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y xvfb ffmpeg > /dev/null 2>&1\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SjAEWSVKluG"
      },
      "outputs": [],
      "source": [
        "!pip install swig > /dev/null 2>&1\n",
        "!pip install gym[box2d] > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATFeXb9FwpyN"
      },
      "source": [
        "Defining logs directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRSVNaFLwuoZ"
      },
      "outputs": [],
      "source": [
        "log_dir = 'logs/rewards'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg-kLs-yKwrI"
      },
      "source": [
        "## **Useful modules import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hQCtuXMK2lQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.optim import Adam\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import cv2\n",
        "import uuid\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "from pyvirtualdisplay import Display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-RkryQaK2JY"
      },
      "source": [
        "## **Utility functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi_YGpgbLEeV"
      },
      "source": [
        "The following code defines a function that could be used to create a MP4 video file from a list of frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcUqTlkpLVO-"
      },
      "outputs": [],
      "source": [
        "def create_mp4_video_from_frames(frames, fps):\n",
        "  temp_video_path='tempfile.mp4'\n",
        "  compressed_path='{}.mp4'.format(str(uuid.uuid4()))\n",
        "\n",
        "  size=(frames[0].shape[1],frames[0].shape[0])\n",
        "  out = cv2.VideoWriter(temp_video_path,cv2.VideoWriter_fourcc(*'mp4v'), fps, size)\n",
        "\n",
        "  for i in range(len(frames)):\n",
        "      out.write(frames[i][...,::-1].copy())  #rgb[...,::-1].copy()\n",
        "  out.release()\n",
        "\n",
        "  os.system(f\"ffmpeg -i {temp_video_path} -vcodec libx264 {compressed_path}\")\n",
        "\n",
        "  os.remove(temp_video_path)\n",
        "\n",
        "  return compressed_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jeb8jUSaL7qW"
      },
      "source": [
        "## **Neural Network Definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btFO7zTVMrje"
      },
      "source": [
        "The following code defines a custom feedforward neural network using PyTorch, the structure is the following:\n",
        "- Three layers: input, hidden and output\n",
        "- The input and output size is a parameter passed in from outside\n",
        "- The hidden size is a hyper-parameter set to 64\n",
        "- RELU is used as the activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0PhiSVUMClk"
      },
      "outputs": [],
      "source": [
        "class CustomNN(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super(CustomNN, self).__init__()\n",
        "    self.input_layer = nn.Linear(input_size, 64)\n",
        "    self.hidden_layer = nn.Linear(64, 64)\n",
        "    self.output_layer = nn.Linear(64, output_size)\n",
        "\n",
        "  def forward(self, obs):\n",
        "    if isinstance(obs, np.ndarray):\n",
        "      obs = torch.tensor(obs, dtype=torch.float)\n",
        "\n",
        "    input_activation = F.relu(self.input_layer(obs))\n",
        "    hidden_activation = F.relu(self.hidden_layer(input_activation))\n",
        "    output = F.relu(self.output_layer(hidden_activation))\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6kEU4ZpNnKS"
      },
      "source": [
        "## **PPO Algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXTyCwhHB2i7"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/davidedomini/proximal-policy-optimization-implementation/main/imgs/PPO-pseudocode.webp\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK5GYjdH_keD"
      },
      "source": [
        "### **Important remarks**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWo_VxSAsdjZ"
      },
      "source": [
        "#### Actor and critic method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCn0cuEuvb2B"
      },
      "source": [
        "*The Actor-Critic is a combination of value-based, and policy-based methods where the Actor controls how our agent behaves using the Policy gradient, and the Critic evaluates how good the action taken by the Agent based on value-function.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqypefwKxjB7"
      },
      "source": [
        "- The Actor-Critic RL aims to find an optimal policy for the agent in an environment using two components: Actor and Critic;\n",
        "- Actor: learns an optimal policy by exploring the environment;\n",
        "- Critic: assesses the value of each action taken by the Actor to determine whether the action will result in a better reward, guiding the Actor for the best course of action to take."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MI4U7yzwyYI"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/davidedomini/proximal-policy-optimization-implementation/main/imgs/ac_method.webp\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAGlM79g7S5R"
      },
      "source": [
        "- The actor takes the current state as input from the environment. Using a neural network, which approximates the policy, it outputs the probabilities of each action for the state;\n",
        "- The critic takes the current state and the actor's outputted actions as inputs and uses this information to estimate the advantage\n",
        "  - *Q(s,a)* represents the expected cumulative reward an agent can expect to receive if it follows a certain policy in a given state;\n",
        "  - *V(s)* represents the expected future reward for a given state, regardless of the action taken;\n",
        "  - If the advantage function for a particular state-action pair is positive, taking that action in that state is expected to yield a better outcome than the average action taken in that state;\n",
        "  - The negative value of the advantage function indicates that the current action is less advantageous than expected, and the agent needs to explore other actions or update the policy to improve the performance;\n",
        "- Finally, the advantage function is backpropagated to both the actor and the critic, allowing both components to continuously update and improve their respective functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bMTudvnsgga"
      },
      "source": [
        "#### Rewards to go\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd9GPvHhgb5E"
      },
      "source": [
        "The reward to go estimates the discounted future reward that can be obtained starting from a given state $S_k$.\n",
        "It can be calculated as follow (where Î³ is a parameter in the range$[0, 1]$):\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKHQO44r_oaf"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/davidedomini/proximal-policy-optimization-implementation/main/imgs/reward-to-go.webp\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm6lZ2JLsjlz"
      },
      "source": [
        "#### Adding exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T06kjBk_w3x8"
      },
      "source": [
        "An exploration factor can be added to the actor's choice of action not by directly using the action chosen by the neural network but by using it to go and sample a new one from a probability distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZLnBZXM_gGp"
      },
      "source": [
        "### **Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-VvgSrqNsoE"
      },
      "outputs": [],
      "source": [
        "class ProximalPolicyOptimization:\n",
        "\n",
        "  def __init__(self, environment):\n",
        "\n",
        "    # Initialize hyperparameters\n",
        "    self.initialize_hyperparameters()\n",
        "\n",
        "    # Extract useful envrironment information\n",
        "    self.environment = environment\n",
        "    self.observation_size = self.environment.observation_space.shape[0]\n",
        "    self.action_size = self.environment.action_space.shape[0]\n",
        "\n",
        "    # ALGORITHM STEP 1: Initialize actor and critic networks\n",
        "    self.actor = CustomNN(self.observation_size, self.action_size)\n",
        "    self.critic = CustomNN(self.observation_size, 1)\n",
        "\n",
        "    # Intialize optimizers\n",
        "    self.actor_optimizer = Adam(self.actor.parameters(), lr=self.learning_rate)\n",
        "    self.critic_optimizer = Adam(self.critic.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    # Initialize distribution for actions selection\n",
        "    self.cov_var = torch.full(size=(self.action_size,), fill_value=0.5)\n",
        "    self.cov_matrix = torch.diag(self.cov_var)\n",
        "\n",
        "    # Initialize tensorboard logger\n",
        "    self.summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "    # Initialize all episodic rewards container\n",
        "    self.all_episodic_rewards = []\n",
        "\n",
        "  def initialize_hyperparameters(self):\n",
        "    self.timesteps_per_batch = 4080\n",
        "    self.max_timesteps_per_episode = 1600\n",
        "    self.gamma = 0.95\n",
        "    self.updates_per_iteration = 5\n",
        "    self.learning_rate = 0.005\n",
        "    self.clip = 0.2 # Value suggested by the paper\n",
        "\n",
        "  def get_action(self, observation, testing=False):\n",
        "\n",
        "    action = self.actor.forward(observation)\n",
        "\n",
        "    # If we are in training we add an exploratory factor\n",
        "    if not testing:\n",
        "      # Create the Multivariate Normal Distribution\n",
        "      distribution = MultivariateNormal(action, self.cov_matrix)\n",
        "\n",
        "      # Sample an action from the distribution and get its log probability\n",
        "      action_with_exploration = distribution.sample()\n",
        "      log_probability = distribution.log_prob(action_with_exploration)\n",
        "\n",
        "      return action_with_exploration.detach().numpy(), log_probability.detach()\n",
        "    else:\n",
        "      return action.detach().numpy(), 0\n",
        "\n",
        "  def evaluate_action(self, batch_observations, batch_actions):\n",
        "\n",
        "    V = self.critic.forward(batch_observations).squeeze()\n",
        "\n",
        "    mean = self.actor.forward(batch_observations)\n",
        "    distribution = MultivariateNormal(mean, self.cov_matrix)\n",
        "    log_probabilities = distribution.log_prob(batch_actions)\n",
        "\n",
        "    return V, log_probabilities\n",
        "\n",
        "  def compute_rewards_to_go(self, batch_rewards):\n",
        "\n",
        "    batch_rtgs = []\n",
        "\n",
        "    for episode_rewards in reversed(batch_rewards):\n",
        "      discounted_reward = 0\n",
        "      for reward in reversed(episode_rewards):\n",
        "        discounted_reward = reward + discounted_reward * self.gamma\n",
        "        batch_rtgs.insert(0, discounted_reward)\n",
        "\n",
        "    batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n",
        "\n",
        "    return batch_rtgs\n",
        "\n",
        "  def log_rewards(self, batch_rewards):\n",
        "    for episode_rewards in batch_rewards:\n",
        "      total_reward = np.sum(episode_rewards)\n",
        "      self.all_episodic_rewards.append(total_reward)\n",
        "\n",
        "  def rollout(self):\n",
        "\n",
        "    # Batch data\n",
        "    batch_observations = []\n",
        "    batch_actions = []\n",
        "    batch_log_probabilities = []\n",
        "    batch_rewards = []\n",
        "    batch_rewards_to_go = []\n",
        "    batch_episodes_lengths = []\n",
        "\n",
        "    time = 0\n",
        "\n",
        "    while time < self.timesteps_per_batch:\n",
        "      episode_rewards = []\n",
        "      observation = self.environment.reset()\n",
        "      done = False\n",
        "\n",
        "      for episode_time in range(self.max_timesteps_per_episode):\n",
        "        time = time + 1\n",
        "        batch_observations.append(observation)\n",
        "\n",
        "        action, log_probability = self.get_action(observation)\n",
        "        observation, reward, done, _ = self.environment.step(action)\n",
        "\n",
        "        episode_rewards.append(reward)\n",
        "        batch_actions.append(action)\n",
        "        batch_log_probabilities.append(log_probability)\n",
        "\n",
        "        if done:\n",
        "          break\n",
        "\n",
        "      batch_episodes_lengths.append(episode_time + 1)\n",
        "      batch_rewards.append(episode_rewards)\n",
        "\n",
        "    # Convert data into tensors\n",
        "    batch_observations = torch.tensor(batch_observations, dtype=torch.float)\n",
        "    batch_actions = torch.tensor(batch_actions, dtype=torch.float)\n",
        "    batch_log_probabilities = torch.tensor(batch_log_probabilities, dtype=torch.float)\n",
        "\n",
        "    # Log episodic rewards\n",
        "    self.log_rewards(batch_rewards)\n",
        "\n",
        "    # ALGORITHM STEP 4: Compute rewards to go\n",
        "    batch_rewards_to_go = self.compute_rewards_to_go(batch_rewards)\n",
        "\n",
        "    return batch_observations, batch_actions, batch_log_probabilities, batch_rewards_to_go, batch_episodes_lengths\n",
        "\n",
        "  def learn(self, total_timesteps):\n",
        "    time = 0\n",
        "    while time < total_timesteps: # ALGORITHM STEP 2: Iterate through time\n",
        "\n",
        "      # ALGORITHM STEP 3: Collect set of trajectories\n",
        "      batch_observations, batch_actions, batch_log_probabilities, batch_rewards_to_go, batch_episodes_lengths = self.rollout()\n",
        "      time = time + np.sum(batch_episodes_lengths)\n",
        "\n",
        "      # ALGORITHM STEP 5: Compute advantages\n",
        "      V, _ = self.evaluate_action(batch_observations, batch_actions) # Compute V_{phi, k}\n",
        "      A_k = batch_rewards_to_go - V.detach()\n",
        "      A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10) # Normalization\n",
        "\n",
        "      for _ in range(self.updates_per_iteration):\n",
        "        # Compute V_{phi} and phi_{theta}(a_t | s_t)\n",
        "        V, current_log_probabilities = self.evaluate_action(batch_observations, batch_actions)\n",
        "\n",
        "        # ALGORITHM STEP 6: Update actor network\n",
        "\n",
        "        # Compute ratios\n",
        "        ratios = torch.exp(current_log_probabilities - batch_log_probabilities)\n",
        "\n",
        "        # Compute surrogate loss functions\n",
        "        surrogate_loss_1 = ratios * A_k\n",
        "        surrogate_loss_2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k\n",
        "\n",
        "        # Compute actor loss\n",
        "        actor_loss = (- torch.min(surrogate_loss_1, surrogate_loss_2)).mean()\n",
        "\n",
        "        # Compute gradients and perform backward propagation for actor network\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward(retain_graph=True)\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # ALGORITHM STEP 7: Update critic network\n",
        "\n",
        "        # Calculate critic loss\n",
        "        critic_loss = nn.MSELoss()(V, batch_rewards_to_go)\n",
        "\n",
        "        # Compute gradients and perform backward propagation for critic network\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "  def visualize_single_episode(self):\n",
        "    frames = []\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    state = self.environment.reset()\n",
        "\n",
        "    while not done:\n",
        "      frame = self.environment.render(mode='rgb_array')\n",
        "      frames.append(frame)\n",
        "\n",
        "      action, _ = self.get_action(state, True)\n",
        "      state, reward, done, _ = self.environment.step(action)\n",
        "      episode_reward += reward\n",
        "\n",
        "    print(f'Total reward: {episode_reward}')\n",
        "\n",
        "    create_mp4_video_from_frames(frames, 30)\n",
        "\n",
        "  def save_neural_networks(self):\n",
        "    torch.save(self.actor.state_dict(), 'actor_nn.pt')\n",
        "    torch.save(self.critic.state_dict(), 'critic_nn.pt')\n",
        "\n",
        "  def load_neural_networks(self, actor_path, critic_path):\n",
        "    self.actor.load_state_dict(torch.load(actor_path))\n",
        "    self.critic.load_state_dict(torch.load(critic_path))\n",
        "\n",
        "  def create_reward_chart(self):\n",
        "    with self.summary_writer.as_default():\n",
        "      for i, r in enumerate(self.all_episodic_rewards):\n",
        "        tf.summary.scalar(\"reward\", r, step=i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFDq6KiSNtAP"
      },
      "source": [
        "## **Algorithm test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8RNM5UhNya3"
      },
      "source": [
        "To test the implemented PPO algorithm we will use a Gym environment called *Bipedal walker*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlJBHPh1nDOI"
      },
      "source": [
        "### **Environment description**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMJf0lRToByx"
      },
      "source": [
        "This is a simple 4-joint walker robot environment, this notebook uses the *normal* version with slightly uneven terrain.\n",
        "\n",
        "- Action space: motor speed values in the [-1, 1] range for each of the 4 joints at both hips and knees;\n",
        "- Observation space: state consists of hull angle speed, angular velocity, horizontal speed, vertical speed, position of joints and joints angular speed, legs contact with ground, and 10 lidar rangefinder measurements. There are no coordinates in the state vector;\n",
        "- Rewards: reward is given for moving forward, totaling 300+ points up to the far end. If the robot falls, it gets -100. Applying motor torque costs a small amount of points. A more optimal agent will get a better score;\n",
        "- Starting state: the walker starts standing at the left end of the terrain with the hull horizontal, and both legs in the same position with a slight knee angle;\n",
        "- Episode termination: the episode will terminate if the hull gets in contact with the ground or if the walker exceeds the right end of the terrain length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aNqxbaGpUx7"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/davidedomini/proximal-policy-optimization-implementation/main/imgs/bipedal_walker.gif\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hpGk5t9nIz8"
      },
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBkicMvYp0C6"
      },
      "source": [
        "The training should last about 15-20 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HcYaYHIAhhC"
      },
      "outputs": [],
      "source": [
        "env = gym.make('BipedalWalker-v3')\n",
        "model = ProximalPolicyOptimization(env)\n",
        "model.learn(300000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK9pKdMpzcwl"
      },
      "outputs": [],
      "source": [
        "model.create_reward_chart()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwEi-ZCTzeli"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl1IFHX1zjcs"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3Q0JK0UAw0f"
      },
      "outputs": [],
      "source": [
        "model.visualize_single_episode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jreaSsesZd_"
      },
      "outputs": [],
      "source": [
        "model.save_neural_networks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg9NVK32n3YI"
      },
      "source": [
        "### **Using a pre-trained NN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01K8SwsqsUes"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/davidedomini/proximal-policy-optimization-implementation/main/pre-trained-nn/actor_nn.pt\n",
        "!wget https://raw.githubusercontent.com/davidedomini/proximal-policy-optimization-implementation/main/pre-trained-nn/critic_nn.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKOBfmsjsH0R"
      },
      "outputs": [],
      "source": [
        "env = gym.make('BipedalWalker-v3')\n",
        "model = ProximalPolicyOptimization(env)\n",
        "model.load_neural_networks('actor_nn.pt', 'critic_nn.pt')\n",
        "model.visualize_single_episode()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result should be something similar to the gif shown below:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/davidedomini/proximal-policy-optimization-implementation/main/imgs/result.gif\" width=400>"
      ],
      "metadata": {
        "id": "-mWI2Km0OPB5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUu+KqxQ2ZJBWR0avx1OY6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}